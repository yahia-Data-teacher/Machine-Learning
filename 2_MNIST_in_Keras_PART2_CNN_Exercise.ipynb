{"cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github", "colab_type": "text"}, "source": ["<a href=\"https://colab.research.google.com/github/yahia-kplr/Machine-Learning/blob/main/2_MNIST_in_Keras_PART2_CNN_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "markdown", "metadata": {"id": "GLWf6RCASZW4"}, "source": ["# Introducing Convolution! What is it?"]}, {"cell_type": "markdown", "metadata": {"id": "fft-Hv_TSZW4"}, "source": ["Before, we built a network that accepts the normalized pixel values of each value and operates soley on those values. What if we could instead feed different features (e.g. **curvature, edges**) of each image into a network, and have the network learn which features are important for classifying an image?\n", "\n", "This possible through convolution! Convolution applies **kernels** (filters) that traverse through each image and generate **feature maps**."]}, {"cell_type": "markdown", "metadata": {"id": "2EXxA4miSZW4"}, "source": ["<img src = 'convolution.gif' >"]}, {"cell_type": "markdown", "metadata": {"id": "7OQSR5PbSZW4"}, "source": ["In the above example, the image is a 5 x 5 matrix and the kernel going over it is a 3 x 3 matrix. A dot product operation takes place between the image and the kernel and the convolved feature is generated. Each kernel in a CNN learns a different characteristic of an image.\n", "\n", "Kernels are often used in photoediting software to apply blurring, edge detection, sharpening, etc."]}, {"cell_type": "markdown", "metadata": {"id": "7vaolViJSZW5"}, "source": ["<img src = 'kernels.png' >"]}, {"cell_type": "markdown", "metadata": {"id": "G1XL4BYYSZW5"}, "source": ["Kernels in deep learning networks are used in similar ways, i.e. highlighting some feature. Combined with a system called **max pooling**, the non-highlighted elements are discarded from each feature map, leaving only the features of interest, reducing the number of learned parameters, and decreasing the computational cost (e.g. system memory)."]}, {"cell_type": "markdown", "metadata": {"id": "ZKBEoh6zSZW5"}, "source": ["<img src = 'max_pooling.png' >"]}, {"cell_type": "markdown", "metadata": {"id": "M05uxl3iSZW5"}, "source": ["We can also take convolutions of convolutions -- we can stack as many convolutions as we want, as long as there are enough pixels to fit a kernel.\n", "\n", "*Warning: What you may find down there in those deep convolutions may not appear recognizable to you.*"]}, {"cell_type": "markdown", "metadata": {"id": "YS-jaFj_SZW5"}, "source": ["## Building a \"Deep\" Convolutional Neural Network"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "W4Bn-VOwSZW5"}, "outputs": [], "source": ["# import some additional tools\n", "\n", "from keras.preprocessing.image import ImageDataGenerator\n", "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten\n", "from tensorflow.keras.layers import BatchNormalization"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "KBdTFGKwSZW6"}, "outputs": [], "source": ["# Reload the MNIST data\n", "# fill here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "jP2FRIe4SZW6", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "9890bf08-b7ea-4029-b179-952a211b5909"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Training matrix shape (60000, 28, 28, 1)\n", "Testing matrix shape (10000, 28, 28, 1)\n"]}], "source": ["# Again, do some formatting\n", "# Except we do not flatten each image into a 784-length vector because we want to perform convolutions first\n", "\n", "X_train = X_train.reshape(60000, 28, 28, 1) #add an additional dimension to represent the single-channel\n", "X_test =  #fill here   (10 000)\n", "\n", "X_train = X_train.astype('float32')         # change integers to 32-bit floating point numbers\n", "X_test = X_test.astype('float32')\n", "\n", "X_train /= 255                              # normalize each value for each pixel for the entire vector for each input\n", "X_test /= 255\n", "\n", "print(\"Training matrix shape\", X_train.shape)\n", "print(\"Testing matrix shape\", X_test.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "lF3cWVClSZW6"}, "outputs": [], "source": ["# one-hot format classes\n", "\n", "nb_classes = 10 # number of unique digits\n", "\n", "Y_train = np_utils.to_categorical(y_train, nb_classes)\n", "Y_test = np_utils.to_categorical(y_test, nb_classes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "_SiMgFZdSZW6"}, "outputs": [], "source": ["model = Sequential()                                 # Linear stacking of layers\n", "\n", "# Convolution Layer 1\n", "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps\n", "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n", "convLayer01 = Activation('relu')                     # activation\n", "model.add(convLayer01)\n", "\n", "# Convolution Layer 2\n", "model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps\n", "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n", "model.add(Activation('relu'))                        # activation\n", "convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n", "model.add(convLayer02)\n", "\n", "# Convolution Layer 3\n", "model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps\n", "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n", "convLayer03 = Activation('relu')                     # activation\n", "model.add(convLayer03)\n", "\n", "# Convolution Layer 4\n", "model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps\n", "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n", "model.add(Activation('relu'))                        # activation\n", "convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n", "model.add(convLayer04)\n", "model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector\n", "\n", "# Fully Connected Layer 5\n", "model.add(Dense(512))                                # 512 FCN nodes\n", "model.add(BatchNormalization())                      # normalization\n", "model.add(Activation('relu'))                        # activation\n", "\n", "# Fully Connected Layer 6                       \n", "model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes\n", "model.add(Dense(10))                                 # final 10 FCN nodes\n", "model.add(Activation('softmax'))                     # softmax activation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "FA0GAXmUSZW6", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "b61e1332-e9e3-4593-95b1-b25ef6563f50"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Model: \"sequential_3\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n", "                                                                 \n", " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n", " ormalization)                                                   \n", "                                                                 \n", " activation_6 (Activation)   (None, 26, 26, 32)        0         \n", "                                                                 \n", " conv2d_2 (Conv2D)           (None, 24, 24, 32)        9248      \n", "                                                                 \n", " batch_normalization_1 (Batc  (None, 24, 24, 32)       128       \n", " hNormalization)                                                 \n", "                                                                 \n", " activation_7 (Activation)   (None, 24, 24, 32)        0         \n", "                                                                 \n", " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n", " )                                                               \n", "                                                                 \n", " conv2d_3 (Conv2D)           (None, 10, 10, 64)        18496     \n", "                                                                 \n", " batch_normalization_2 (Batc  (None, 10, 10, 64)       256       \n", " hNormalization)                                                 \n", "                                                                 \n", " activation_8 (Activation)   (None, 10, 10, 64)        0         \n", "                                                                 \n", " conv2d_4 (Conv2D)           (None, 8, 8, 64)          36928     \n", "                                                                 \n", " batch_normalization_3 (Batc  (None, 8, 8, 64)         256       \n", " hNormalization)                                                 \n", "                                                                 \n", " activation_9 (Activation)   (None, 8, 8, 64)          0         \n", "                                                                 \n", " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n", " 2D)                                                             \n", "                                                                 \n", " flatten (Flatten)           (None, 1024)              0         \n", "                                                                 \n", " dense_6 (Dense)             (None, 512)               524800    \n", "                                                                 \n", " batch_normalization_4 (Batc  (None, 512)              2048      \n", " hNormalization)                                                 \n", "                                                                 \n", " activation_10 (Activation)  (None, 512)               0         \n", "                                                                 \n", " dropout_4 (Dropout)         (None, 512)               0         \n", "                                                                 \n", " dense_7 (Dense)             (None, 10)                5130      \n", "                                                                 \n", " activation_11 (Activation)  (None, 10)                0         \n", "                                                                 \n", "=================================================================\n", "Total params: 597,738\n", "Trainable params: 596,330\n", "Non-trainable params: 1,408\n", "_________________________________________________________________\n"]}], "source": ["# Informations sur le mod\u00e8le\n", "#fill here "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "HPnh4893SZW7"}, "outputs": [], "source": ["# we'll use the same optimizer\n", "\n", "#fill here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "bdKXsOBHSZW7"}, "outputs": [], "source": ["# data augmentation prevents overfitting by slightly changing the data randomly\n", "# Keras has a great built-in feature to do automatic augmentation\n", "\n", "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n", "                         height_shift_range=0.08, zoom_range=0.08)\n", "\n", "test_gen = ImageDataGenerator()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "zP4YjSIeSZW7"}, "outputs": [], "source": ["# We can then feed our augmented data in batches\n", "# Besides loss function considerations as before, this method actually results in significant memory savings\n", "# because we are actually LOADING the data into the network in batches before processing each batch\n", "\n", "# Before the data was all loaded into memory, but then processed in batches.\n", "\n", "train_generator = gen.flow(X_train, Y_train, batch_size=128)\n", "test_generator = test_gen.flow(X_test, Y_test, batch_size=128)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "nUR7WD0WSZW7", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "c891ab31-3e0b-4e96-ffa9-171bd62a334e"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n", "  import sys\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Epoch 1/5\n", "468/468 [==============================] - 198s 421ms/step - loss: 0.1370 - accuracy: 0.9573 - val_loss: 0.1759 - val_accuracy: 0.9431\n", "Epoch 2/5\n", "468/468 [==============================] - 196s 419ms/step - loss: 0.0495 - accuracy: 0.9848 - val_loss: 0.0504 - val_accuracy: 0.9836\n", "Epoch 3/5\n", "468/468 [==============================] - 201s 430ms/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 0.0624 - val_accuracy: 0.9803\n", "Epoch 4/5\n", "468/468 [==============================] - 200s 427ms/step - loss: 0.0358 - accuracy: 0.9890 - val_loss: 0.0275 - val_accuracy: 0.9914\n", "Epoch 5/5\n", "468/468 [==============================] - 195s 417ms/step - loss: 0.0313 - accuracy: 0.9902 - val_loss: 0.0290 - val_accuracy: 0.9905\n"]}, {"output_type": "execute_result", "data": {"text/plain": ["<keras.callbacks.History at 0x7f4b4967bad0>"]}, "metadata": {}, "execution_count": 60}], "source": ["# We can now train our model which is fed data by our batch loader\n", "# Steps per epoch should always be total size of the set divided by the batch size\n", "\n", "# SIGNIFICANT MEMORY SAVINGS (important for larger, deeper networks)\n", "\n", "model.fit_generator(train_generator, steps_per_epoch=#fill here,  \n", "                    epochs=5, \n", "                    verbose=1, \n", "                    validation_data=test_generator, \n", "                    validation_steps=10000//128)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ygJmvq1JSZW7", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "23608a53-0a21-4ce0-c687-9fed4a197e6f"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["313/313 [==============================] - 9s 27ms/step - loss: 0.0289 - accuracy: 0.9905\n", "Test score: 0.02893815003335476\n", "Test accuracy: 0.9904999732971191\n"]}], "source": ["# Model Score\n", "# Fill here"]}, {"cell_type": "markdown", "metadata": {"id": "vtp6CFIxSZW7"}, "source": ["## Great results! \n", "\n", "But wouldn't it be nice if we could visualize those convolutions so that we can see what the model is seeing?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "id": "b6wYkXXYSZW8"}, "outputs": [], "source": ["from keras import backend as K\n", "\n", "# choose any image to want by specifying the index\n", "img = X_test[3]\n", "img = np.expand_dims(img, axis=0) # Keras requires the image to be in 4D, so we add an extra dimension to it.\n", "\n", "# Not important to understand how this function work -- It just plots a convolution layer\n", "\n", "def visualize(layer):\n", "    inputs = [K.learning_phase()] + model.inputs\n", "    \n", "    _convout1_f = K.function(inputs, [layer.output])\n", "    \n", "    def convout1_f(X):\n", "        # The [0] is to disable the training phase flag\n", "        return _convout1_f([0] + [X])\n", "\n", "    convolutions = convout1_f(img)\n", "    convolutions = np.squeeze(convolutions)\n", "\n", "    print ('Shape of conv:', convolutions.shape)\n", "    \n", "    m = convolutions.shape[2]\n", "    n = int(np.ceil(np.sqrt(m)))\n", "    \n", "    # Visualization of each filter of the layer\n", "    fig = plt.figure(figsize=(15,12))\n", "    for i in range(m):\n", "        ax = fig.add_subplot(n,n,i+1)\n", "        ax.imshow(convolutions[:,:,i], cmap='gray')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "_0iMXDXNSZW8", "colab": {"base_uri": "https://localhost:8080/", "height": 555}, "outputId": "58e4771f-dbd3-4b06-c6de-f343d414cb13"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["<matplotlib.image.AxesImage at 0x7f4b4430e490>"]}, "metadata": {}, "execution_count": 63}, {"output_type": "display_data", "data": {"text/plain": ["<Figure size 648x648 with 1 Axes>"], "image/png": "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\n"}, "metadata": {"needs_background": "light"}}], "source": ["plt.figure()\n", "plt.imshow(X_test[3].reshape(28,28), cmap='gray', interpolation='none')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "_6v_Eh-KSZW8"}, "outputs": [], "source": ["visualize(convLayer01) # visualize first set of feature maps"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "34eP3XyXSZW8"}, "outputs": [], "source": ["visualize(convLayer02) # visualize second set of feature maps"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hblItTPJSZW8"}, "outputs": [], "source": ["visualize(convLayer03)# visualize third set of feature maps"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "t3ZUhp6CSZW8"}, "outputs": [], "source": ["visualize(convLayer04)# visualize fourth set of feature maps"]}, {"cell_type": "markdown", "metadata": {"id": "KoTdmnwmSZW8"}, "source": ["#### For a 3D visualization of a very similar network, visit http://scs.ryerson.ca/~aharley/vis/conv/"]}], "metadata": {"kernelspec": {"display_name": "Tensorflow (GPU)", "language": "python", "name": "py3.6-tfgpu"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 0}