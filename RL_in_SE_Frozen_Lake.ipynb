{"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github", "colab_type": "text"}, "source": ["<a href=\"https://colab.research.google.com/github/yahia-kplr/Machine-Learning/blob/main/RL_in_SE_Frozen_Lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "markdown", "metadata": {"id": "54AIvDov_7aa"}, "source": ["## Step 1: Install the dependencies on Google Colab"]}, {"cell_type": "code", "metadata": {"id": "gxxpHDIs_lvg", "outputId": "df46c564-074a-4e3d-9a88-ce8fb27a1c12", "colab": {"base_uri": "https://localhost:8080/"}}, "source": ["!pip install numpy\n", "!pip install openai-gym"], "execution_count": 17, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n", "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n", "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n", "\u001b[31mERROR: Could not find a version that satisfies the requirement openai-gym (from versions: none)\u001b[0m\n", "\u001b[31mERROR: No matching distribution found for openai-gym\u001b[0m\n"]}]}, {"cell_type": "code", "metadata": {"id": "oU8zRXv8QHlm"}, "source": ["#import the required libraries.\n", "import numpy as np\n", "import gym\n", "import random"], "execution_count": 18, "outputs": []}, {"cell_type": "code", "metadata": {"id": "mh9jBR_cQ5_a"}, "source": ["#create the environment usign OpenAI Gym\n", "env = gym.make(\"FrozenLake-v0\")"], "execution_count": 19, "outputs": []}, {"cell_type": "markdown", "source": ["Q* Learning with FrozenLake 4x4 \n", "\n", "we'll implement an agent <b>that plays FrozenLake.</b>\n", "\n", "![alt text](http://simoninithomas.com/drlc/Qlearning/frozenlake4x4.png)\n", "\n", "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, **so you won't always move in the direction you intend (stochastic environment)**"], "metadata": {"id": "yabnPJ07iGbq"}}, {"cell_type": "markdown", "metadata": {"id": "JEtXMldxQ7uw"}, "source": ["## Step 2: Create the Q-table and initialize it"]}, {"cell_type": "code", "metadata": {"id": "Uc0xDVd_Q-C8", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "3ef526ee-bcfa-409f-aef9-52ddbabe09d6"}, "source": ["action_size = env.action_space.n\n", "state_size = env.observation_space.n\n", "\n", "print(f\"Action Space : {action_size} | State Space: {state_size}\")"], "execution_count": 20, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Action Space : 4 | State Space: 16\n"]}]}, {"cell_type": "code", "metadata": {"id": "0J_GfR-p25bq", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "b51f5f86-fe67-40d5-e45b-d1f493e9b2e5"}, "source": ["qtable = np.zeros((state_size, action_size))\n", "print(qtable.shape)"], "execution_count": 21, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["(16, 4)\n"]}]}, {"cell_type": "markdown", "metadata": {"id": "9DbAR9J_3DXa"}, "source": ["## Step 3: Create Required Hyperparameters"]}, {"cell_type": "code", "metadata": {"id": "dBHB8MIl71Aw"}, "source": ["total_episodes = 15000        # Total episodes\n", "learning_rate = 0.8           # Learning rate\n", "max_steps = 99                # Max steps per episode\n", "gamma = 0.95                  # Discounting rate\n", "\n", "# Exploration parameters\n", "epsilon = 1.0                 # Exploration rate\n", "max_epsilon = 1.0             # Exploration probability at start\n", "min_epsilon = 0.01            # Minimum exploration probability \n", "decay_rate = 0.005            # Exponential decay rate for exploration prob"], "execution_count": 22, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "xqu-5j9B7qmy"}, "source": ["## Step 4 : Q-Learning Algorithm"]}, {"cell_type": "code", "metadata": {"id": "YJYnA88a3TmG", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "fdff7789-03c3-412d-ac59-f80faa951c9f"}, "source": ["# List of rewards\n", "rewards = []\n", "\n", "#until learning is stopped\n", "for episode in range(total_episodes):\n", "    # Reset the environment\n", "    state = env.reset()\n", "    step = 0\n", "    done = False\n", "    total_rewards = 0\n", "    \n", "    for step in range(max_steps):\n", "        #Choose an action a in the current world state (s)\n", "        exp_exp_tradeoff = random.uniform(0, 1)\n", "        \n", "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n", "        if exp_exp_tradeoff > epsilon:\n", "            action = np.argmax(qtable[state,:])\n", "\n", "        # Else doing a random choice --> exploration\n", "        else:\n", "            action = env.action_space.sample()\n", "\n", "        # Take the action (a) and observe the outcome state(s') and reward (r)\n", "        new_state, reward, done, info = env.step(action)\n", "\n", "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n", "        # qtable[new_state,:] : all the actions we can take from new state\n", "        qtable[state, action] = qtable[state, action] + learning_rate * \\\n", "                                                        (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n", "        \n", "        total_rewards += reward\n", "        \n", "        # Our new state is state\n", "        state = new_state\n", "        \n", "        # If done (if we're dead) : finish episode\n", "        if done == True: \n", "            break\n", "        \n", "    # Reduce epsilon (because we need less and less exploration)\n", "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n", "\n", "    rewards.append(total_rewards)\n", "\n", "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n", "print(qtable)"], "execution_count": 23, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Score over time: 0.0\n", "[[0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]\n", " [0. 0. 0. 0.]]\n"]}]}, {"cell_type": "markdown", "source": ["## Step 5 : Show Episodes"], "metadata": {"id": "M5U5yfrykRol"}}, {"cell_type": "code", "metadata": {"id": "gQvoFSsr3TkM", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "dfe26de1-40e8-4642-e0c2-5b12d47f1152"}, "source": ["for episode in range(5):\n", "    state = env.reset()\n", "    step = 0\n", "    done = False\n", "    print(\"****************************************************\")\n", "    print(\"EPISODE \", episode)\n", "\n", "    for step in range(max_steps):\n", "        \n", "        # Take the action (index) that have the maximum expected future reward given that state\n", "        action = np.argmax(qtable[state,:])\n", "        \n", "        new_state, reward, done, info = env.step(action)\n", "        \n", "        if done:\n", "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n", "            env.render()\n", "            \n", "            # We print the number of step it took.\n", "            print(\"Number of steps\", step)\n", "            break\n", "        state = new_state\n", "env.close()"], "execution_count": 24, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["****************************************************\n", "EPISODE  0\n", "  (Left)\n", "SFFF\n", "FHFH\n", "FFFH\n", "\u001b[41mH\u001b[0mFFG\n", "Number of steps 54\n", "****************************************************\n", "EPISODE  1\n", "  (Left)\n", "SFFF\n", "FHFH\n", "FFFH\n", "\u001b[41mH\u001b[0mFFG\n", "Number of steps 8\n", "****************************************************\n", "EPISODE  2\n", "  (Left)\n", "SFFF\n", "FHFH\n", "FFFH\n", "\u001b[41mH\u001b[0mFFG\n", "Number of steps 5\n", "****************************************************\n", "EPISODE  3\n", "  (Left)\n", "SFFF\n", "FHFH\n", "FFFH\n", "\u001b[41mH\u001b[0mFFG\n", "Number of steps 11\n", "****************************************************\n", "EPISODE  4\n", "  (Left)\n", "SFFF\n", "FHFH\n", "FFFH\n", "\u001b[41mH\u001b[0mFFG\n", "Number of steps 6\n"]}]}]}