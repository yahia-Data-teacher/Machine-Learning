{"cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github", "colab_type": "text"}, "source": ["<a href=\"https://colab.research.google.com/github/yahia-kplr/Machine-Learning/blob/main/RL_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "wkd375upWYok"}, "outputs": [], "source": ["# Copyright 2022 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n", "# \n", "# Licensed under the MIT License. You may not use this file except in compliance\n", "# with the License. Use and/or modification of this code outside of 6.S191 must\n", "# reference:\n", "#\n", "# \u00a9 MIT 6.S191: Introduction to Deep Learning\n", "# http://introtodeeplearning.com\n", "#"]}, {"cell_type": "markdown", "metadata": {"id": "WoXYKhfZMHiw"}, "source": ["#Reinforcement Learning\n", "\n", "L'apprentissage par renforcement (RL) est un sous-ensemble de l'apprentissage automatique qui pose les probl\u00e8mes d'apprentissage comme des interactions entre les agents et les environnements. Il suppose souvent que les agents n'ont aucune connaissance pr\u00e9alable du monde et qu'ils doivent apprendre \u00e0 naviguer dans les environnements en optimisant une fonction de r\u00e9compense. Dans un environnement, un agent peut entreprendre certaines actions et recevoir un retour, sous la forme de r\u00e9compenses positives ou n\u00e9gatives, par rapport \u00e0 sa d\u00e9cision. En tant que telle, la boucle de r\u00e9troaction d'un agent s'apparente quelque peu \u00e0 l'id\u00e9e d'\"essais et erreurs\", ou \u00e0 la mani\u00e8re dont un enfant peut apprendre \u00e0 distinguer les \"bonnes\" et les \"mauvaises\" actions.\n", "\n", "En termes pratiques, notre agent RL interagira avec l'environnement en effectuant une action \u00e0 chaque pas de temps, en recevant une r\u00e9compense correspondante et en mettant \u00e0 jour son \u00e9tat en fonction de ce qu'il a \"appris\".  \n", "\n", "![texte alt](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n", "\n", "Bien que l'objectif ultime de l'apprentissage par renforcement soit d'apprendre aux agents \u00e0 agir dans le monde r\u00e9el et physique, les environnements simul\u00e9s - comme les jeux et les moteurs de simulation - constituent un terrain d'essai pratique pour le d\u00e9veloppement d'algorithmes et d'agents d'apprentissage par renforcement.\n", "\n", "Dans les laboratoires pr\u00e9c\u00e9dents, nous avons explor\u00e9 les t\u00e2ches d'apprentissage supervis\u00e9 (avec LSTMs, CNNs) et non supervis\u00e9 / semi-supervis\u00e9 (avec VAEs). L'apprentissage par renforcement est fondamentalement diff\u00e9rent, dans la mesure o\u00f9 nous formons un algorithme d'apprentissage profond pour r\u00e9gir les actions de notre agent RL, qui essaie, dans son environnement, de trouver la mani\u00e8re optimale d'atteindre un objectif. L'objectif de la formation d'un agent RL est de d\u00e9terminer la meilleure \u00e9tape \u00e0 suivre pour obtenir le meilleur gain ou rendement final. Dans ce laboratoire, nous nous concentrons sur la construction d'un algorithme d'apprentissage par renforcement pour ma\u00eetriser deux environnements diff\u00e9rents avec une complexit\u00e9 variable. \n", "\n", "1.   **Cartpole** :   \u00c9quilibrer un poteau, d\u00e9passant d'un chariot, dans une position verticale en d\u00e9pla\u00e7ant seulement la base \u00e0 gauche ou \u00e0 droite. Environnement avec un espace d'observation de faible dimension.\n", "\n", "\n", "\n", "C'est parti ! Tout d'abord, nous allons importer TensorFlow, le paquetage du cours et certaines d\u00e9pendances."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "KR9QHuleJ9Bh"}, "outputs": [], "source": ["# Import Tensorflow 2.0\n", "%tensorflow_version 2.x\n", "import tensorflow as tf\n", "\n", "gpus = tf.config.experimental.list_physical_devices('GPU')\n", "if gpus:\n", "    for gpu in gpus:\n", "        tf.config.experimental.set_memory_growth(gpu, True)\n", "\n", "# Download and import the MIT 6.S191 package\n", "!printf \"Installing MIT deep learning package... \"\n", "!pip install --upgrade git+https://github.com/aamini/introtodeeplearning.git &> /dev/null\n", "!echo \"Done\""]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "EvdePP-VyVWp"}, "outputs": [], "source": ["#Install some dependencies for visualizing the agents\n", "!apt-get install -y xvfb python-opengl x11-utils &> /dev/null\n", "!pip install gym pyvirtualdisplay scikit-video ffio pyrender &> /dev/null\n", "!pip install tensorflow_probability==0.12.0 &> /dev/null\n", "import os\n", "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n", "\n", "import numpy as np\n", "import matplotlib, cv2\n", "import matplotlib.pyplot as plt\n", "import base64, io, os, time, gym\n", "import IPython, functools\n", "import time\n", "from tqdm import tqdm\n", "import tensorflow_probability as tfp\n", "\n", "import mitdeeplearning as mdl"]}, {"cell_type": "markdown", "metadata": {"id": "zmrHSiXKTXTY"}, "source": ["Avant de nous y plonger, prenons un peu de recul et d\u00e9crivons notre approche, qui est g\u00e9n\u00e9ralement applicable aux probl\u00e8mes d'apprentissage par renforcement en g\u00e9n\u00e9ral :\n", "\n", "1. **Initialiser notre environnement et notre agent** : ici nous allons d\u00e9crire les diff\u00e9rentes observations et actions que l'agent peut faire dans l'environnement.\n", "2. **D\u00e9finir la m\u00e9moire de notre agent** : cela va permettre \u00e0 l'agent de se souvenir de ses actions, observations et r\u00e9compenses pass\u00e9es.\n", "3. **D\u00e9finir une fonction de r\u00e9compense** : d\u00e9crit la r\u00e9compense associ\u00e9e \u00e0 une action ou une s\u00e9quence d'actions.\n", "4. **D\u00e9finir l'algorithme d'apprentissage** : il sera utilis\u00e9 pour renforcer les bons comportements de l'agent et d\u00e9courager les mauvais comportements.\n"]}, {"cell_type": "markdown", "metadata": {"id": "UT7YL8KBJIIc"}, "source": ["# Cartpole\n", "\n", "## 1 D\u00e9finir l'environnement et l'agent de Cartpole\n", "\n", "### Environnement \n", "\n", "Afin de mod\u00e9liser l'environnement de la t\u00e2che Cartpole, nous utiliserons une bo\u00eete \u00e0 outils d\u00e9velopp\u00e9e par OpenAI appel\u00e9e [OpenAI Gym] (https://gym.openai.com/). Elle fournit plusieurs environnements pr\u00e9d\u00e9finis pour l'entra\u00eenement et le test des agents d'apprentissage par renforcement, y compris ceux des t\u00e2ches classiques de contr\u00f4le physique, des jeux vid\u00e9o Atari et des simulations robotiques. Pour acc\u00e9der \u00e0 l'environnement Cartpole, nous pouvons utiliser `env = gym.make(\"CartPole-v0\")`, auquel nous avons eu acc\u00e8s lorsque nous avons import\u00e9 le paquet `gym`. Nous pouvons instancier diff\u00e9rents [environnements] (https://gym.openai.com/envs/#classic_control) en passant le nom de l'environnement \u00e0 la fonction `make`.\n", "\n", "Un probl\u00e8me que nous pouvons rencontrer lors du d\u00e9veloppement d'algorithmes de RL est que de nombreux aspects du processus d'apprentissage sont intrins\u00e8quement al\u00e9atoires : l'initialisation des \u00e9tats du jeu, les changements dans l'environnement et les actions de l'agent. En tant que tel, il peut \u00eatre utile de d\u00e9finir une \"graine\" initiale pour l'environnement afin d'assurer un certain niveau de reproductibilit\u00e9. Tout comme vous pourriez utiliser `numpy.random.seed`, nous pouvons appeler la fonction comparable dans gym, `seed`, avec notre environnement d\u00e9fini pour s'assurer que les variables al\u00e9atoires de l'environnement sont initialis\u00e9es de la m\u00eame mani\u00e8re \u00e0 chaque fois.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "quv9SC0iIYFm"}, "outputs": [], "source": ["### Instantiate the Cartpole environment ###\n", "\n", "env = gym.make(\"CartPole-v1\")\n", "env.seed(1)"]}, {"cell_type": "markdown", "metadata": {"id": "mhEITUcKK455"}, "source": ["Dans Cartpole, un poteau est attach\u00e9 par une articulation non actionn\u00e9e \u00e0 un chariot, qui se d\u00e9place le long d'une piste sans frottement. Le poteau commence debout et le but est de l'emp\u00eacher de tomber. Le syst\u00e8me est contr\u00f4l\u00e9 en appliquant une force de +1 ou -1 au chariot. Une r\u00e9compense de +1 est fournie pour chaque pas de temps pendant lequel le poteau reste debout. L'\u00e9pisode se termine lorsque le poteau est \u00e0 plus de 15 degr\u00e9s de la verticale ou que le chariot se d\u00e9place \u00e0 plus de 2,4 unit\u00e9s du centre de la piste. Un r\u00e9sum\u00e9 visuel de l'environnement cartpole est illustr\u00e9 ci-dessous\u00a0:\n", "\n", "<img width=\"400px\" src=\"https://danielpiedrahita.files.wordpress.com/2017/02/cart-pole.png\"></img>\n", "\n", "Compte tenu de cette configuration de l'environnement et de l'objectif du jeu, nous pouvons r\u00e9fl\u00e9chir \u00e0\u00a0: 1) quelles observations aident \u00e0 d\u00e9finir l'\u00e9tat de l'environnement\u00a0; 2) quelles actions l'agent peut entreprendre.\n", "\n", "Consid\u00e9rons d'abord l'espace d'observation. Dans cet environnement Cartpole, nos observations sont\u00a0:\n", "\n", "1. Emplacement du chariot\n", "2. Vitesse du chariot\n", "3. Angle de poteau\n", "4. Taux de rotation des p\u00f4les\n", "\n", "On peut confirmer la taille de l'espace en interrogeant l'espace d'observation de l'environnement :"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "UVJaEcbdIX82"}, "outputs": [], "source": ["n_observations = env.observation_space\n", "print(\"Environment has observation space =\", n_observations)"]}, {"cell_type": "markdown", "metadata": {"id": "ZibGgjrALgPM"}, "source": ["Deuxi\u00e8mement, nous consid\u00e9rons l'espace d'action. A chaque pas de temps, l'agent peut se d\u00e9placer soit \u00e0 droite soit \u00e0 gauche. L\u00e0 encore, nous pouvons confirmer la taille de l'espace d'action en interrogeant l'environnement :"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "qc9SIPxBIXrm"}, "outputs": [], "source": ["n_actions = env.action_space.n\n", "print(\"Number of possible actions that the agent can choose from =\", n_actions)"]}, {"cell_type": "markdown", "metadata": {"id": "pPfHME8aRKkb"}, "source": ["### 1.Agent Cartpole\n", "\n", "Maintenant que nous avons instanci\u00e9 l'environnement et compris la dimensionnalit\u00e9 des espaces d'observation et d'action, nous sommes pr\u00eats \u00e0 d\u00e9finir notre agent. Dans l'apprentissage par renforcement profond, un r\u00e9seau neuronal profond d\u00e9finit l'agent. Ce r\u00e9seau prendra en entr\u00e9e une observation de l'environnement et produira en sortie la probabilit\u00e9 de prendre chacune des actions possibles. Puisque Cartpole est d\u00e9fini par un espace d'observation de faible dimension, un simple r\u00e9seau de neurones de type feed-forward devrait bien fonctionner pour notre agent. Nous allons le d\u00e9finir en utilisant l'API `Sequential`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "W-o_XK4oQ4eu"}, "outputs": [], "source": ["### Define the Cartpole agent ###\n", "\n", "# Defines a feed-forward neural network\n", "def create_cartpole_model():\n", "    model = tf.keras.models.Sequential([\n", "        # First Dense layer\n", "        tf.keras.layers.Dense(units=32, activation='relu'),\n", "        \n", "        # TODO: Define the last Dense layer, which will provide the network's output.\n", "        # Think about the space the agent needs to act in!\n", "        tf.keras.layers.Dense(units=n_actions, activation=None) # TODO\n", "        # ['''TODO''' Dense layer to output action probabilities]\n", "    ])\n", "    return model\n", "\n", "cartpole_model = create_cartpole_model()"]}, {"cell_type": "markdown", "metadata": {"id": "d5D5NSIYS2IW"}, "source": ["Maintenant que nous avons d\u00e9fini l'architecture de base du r\u00e9seau, nous allons d\u00e9finir une *fonction d'action* qui ex\u00e9cute un passage vers l'avant \u00e0 travers le r\u00e9seau, \u00e9tant donn\u00e9 un ensemble d'observations, et \u00e9chantillonne la sortie. Cet \u00e9chantillonnage des probabilit\u00e9s de sortie sera utilis\u00e9 pour s\u00e9lectionner la prochaine action de l'agent. Nous ajouterons \u00e9galement un support pour que la fonction `choose_action` puisse traiter soit une seule observation, soit un lot d'observations.\n", "\n", "**Nous utiliserons cette fonction pour l'apprentissage d'algorithmes de contr\u00f4le pour Cartpole, mais elle est \u00e9galement applicable \u00e0 d'autres t\u00e2ches de RL."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "E_vVZRr8Q4R_"}, "outputs": [], "source": ["### Define the agent's action function ###\n", "\n", "# Function that takes observations as input, executes a forward pass through model, \n", "#   and outputs a sampled action.\n", "# Arguments:\n", "#   model: the network that defines our agent\n", "#   observation: observation(s) which is/are fed as input to the model\n", "#   single: flag as to whether we are handling a single observation or batch of\n", "#     observations, provided as an np.array\n", "# Returns:\n", "#   action: choice of agent action\n", "def choose_action(model, observation, single=True):\n", "    # add batch dimension to the observation if only a single example was provided\n", "    observation = np.expand_dims(observation, axis=0) if single else observation\n", "\n", "    '''TODO: feed the observations through the model to predict the log probabilities of each possible action.'''\n", "    logits = model.predict(observation) # TODO\n", "    # logits = model.predict('''TODO''')\n", "\n", "    '''TODO: Choose an action from the categorical distribution defined by the log \n", "       probabilities of each possible action.'''\n", "    action = tf.random.categorical(logits, num_samples=1)\n", "    # action = ['''TODO''']\n", "\n", "    action = action.numpy().flatten()\n", "\n", "    return action[0] if single else action"]}, {"cell_type": "markdown", "metadata": {"id": "_tR9uAWcTnkr"}, "source": ["## 2 D\u00e9finir la m\u00e9moire de l'agent\n", "\n", "Maintenant que nous avons instanci\u00e9 l'environnement et d\u00e9fini l'architecture du r\u00e9seau d'agents et la fonction d'action, nous sommes pr\u00eats \u00e0 passer \u00e0 l'\u00e9tape suivante de notre workflow RL :\n", "1. **Initialiser notre environnement et notre agent** : nous allons d\u00e9crire ici les diff\u00e9rentes observations et actions que l'agent peut effectuer dans l'environnement.\n", "2. **D\u00e9finir la m\u00e9moire de notre agent** : cela permettra \u00e0 l'agent de se souvenir de ses actions, observations et r\u00e9compenses pass\u00e9es.\n", "3. **D\u00e9finir l'algorithme d'apprentissage** : il sera utilis\u00e9 pour renforcer les bons comportements de l'agent et d\u00e9courager les mauvais.\n", "\n", "Dans l'apprentissage par renforcement, l'apprentissage a lieu parall\u00e8lement \u00e0 l'action de l'agent dans l'environnement ; un *\u00e9pisode* d\u00e9signe une s\u00e9quence d'actions qui aboutit \u00e0 un \u00e9tat final, tel que la chute du poteau ou le crash du chariot. L'agent devra se souvenir de toutes ses observations et actions, de sorte qu'\u00e0 la fin d'un \u00e9pisode, il puisse apprendre \u00e0 \"renforcer\" les bonnes actions et \u00e0 punir les actions ind\u00e9sirables via l'entra\u00eenement. Notre premi\u00e8re \u00e9tape est de d\u00e9finir un simple tampon `Memory` qui contient les observations de l'agent, ses actions, et les r\u00e9compenses re\u00e7ues pour un \u00e9pisode donn\u00e9. Nous ajouterons \u00e9galement le support pour combiner une liste d'objets `Memory` en un seul `Memory`. Ceci sera tr\u00e8s utile pour le batching, ce qui vous aidera \u00e0 acc\u00e9l\u00e9rer l'entra\u00eenement plus tard dans le laboratoire.\n", "\n", "**Une fois de plus, notez la modularit\u00e9 de cette m\u00e9moire tampon - elle peut et sera appliqu\u00e9e \u00e0 d'autres t\u00e2ches de RL aussi!**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "8MM6JwXVQ4JG"}, "outputs": [], "source": ["### Agent Memory ###\n", "\n", "class Memory:\n", "    def __init__(self): \n", "        self.clear()\n", "\n", "  # Resets/restarts the memory buffer\n", "    def clear(self): \n", "        self.observations = []\n", "        self.actions = []\n", "        self.rewards = []\n", "\n", "  # Add observations, actions, rewards to memory\n", "    def add_to_memory(self, new_observation, new_action, new_reward): \n", "        self.observations.append(new_observation)\n", "        '''TODO: update the list of actions with new action'''\n", "        self.actions.append(new_action) # TODO\n", "        # ['''TODO''']\n", "        '''TODO: update the list of rewards with new reward'''\n", "        self.rewards.append(new_reward) # TODO\n", "        # ['''TODO''']\n", "\n", "    def __len__(self):\n", "        return len(self.actions)\n", "\n", "# Instantiate a single Memory buffer\n", "memory = Memory()"]}, {"cell_type": "markdown", "metadata": {"id": "D4YhtPaUVj5m"}, "source": ["## 3 Fonction de r\u00e9compense\n", "\n", "Nous sommes presque pr\u00eats \u00e0 commencer l'algorithme d'apprentissage de notre agent ! L'\u00e9tape suivante consiste \u00e0 calculer les r\u00e9compenses de notre agent lorsqu'il agit dans l'environnement. Puisque nous (et l'agent) ne savons pas si et quand le jeu ou la t\u00e2che se terminera (c'est-\u00e0-dire quand le poteau tombera), il est utile de mettre l'accent sur l'obtention de r\u00e9compenses **maintenant** plut\u00f4t que plus tard dans le futur - c'est l'id\u00e9e d'actualisation. C'est un concept similaire \u00e0 celui de l'actualisation de l'argent dans le cas des int\u00e9r\u00eats. Rappelez-vous du cours, nous utilisons l'actualisation des r\u00e9compenses pour privil\u00e9gier l'obtention des r\u00e9compenses maintenant plut\u00f4t que plus tard dans le futur. L'id\u00e9e d'actualiser les r\u00e9compenses est similaire \u00e0 l'actualisation de l'argent dans le cas des int\u00e9r\u00eats.\n", "\n", "Pour calculer la r\u00e9compense cumul\u00e9e attendue, appel\u00e9e **rendement**, \u00e0 un moment donn\u00e9 d'un \u00e9pisode d'apprentissage, nous additionnons les r\u00e9compenses actualis\u00e9es attendues \u00e0 ce moment $t$, au sein d'un \u00e9pisode d'apprentissage, et en nous projetant dans le futur. Nous d\u00e9finissons le rendement (r\u00e9compense cumulative) \u00e0 un pas de temps $t$, $R_{t}$ comme :\n", "\n", ">$R_{t}=\\sum_{k=0}^\\infty\\gamma^kr_{t+k}$\n", "\n", "o\u00f9 $0 < \\gamma < 1$ est le facteur d'actualisation et $r_{t}$ est la r\u00e9compense au pas de temps $t$, et l'indice $k$ incr\u00e9mente la projection dans le futur au sein d'un seul \u00e9pisode d'apprentissage. Intuitivement, on peut penser que cette fonction d\u00e9pr\u00e9cie toute r\u00e9compense re\u00e7ue \u00e0 des \u00e9tapes temporelles ult\u00e9rieures, ce qui obligera l'agent \u00e0 donner la priorit\u00e9 \u00e0 l'obtention de r\u00e9compenses maintenant. Puisque nous ne pouvons pas \u00e9tendre les \u00e9pisodes \u00e0 l'infini, en pratique, le calcul sera limit\u00e9 au nombre de pas de temps dans un \u00e9pisode - apr\u00e8s quoi la r\u00e9compense est suppos\u00e9e \u00eatre nulle.\n", "\n", "Prenez note de la forme de cette somme -- nous devrons faire preuve d'ing\u00e9niosit\u00e9 dans l'impl\u00e9mentation de cette fonction. Plus pr\u00e9cis\u00e9ment, nous devrons initialiser un tableau de z\u00e9ros, dont la longueur correspond au nombre de pas de temps, et le remplir avec les valeurs r\u00e9elles de la r\u00e9compense actualis\u00e9e au fur et \u00e0 mesure que nous parcourons les r\u00e9compenses de l'\u00e9pisode, qui auront \u00e9t\u00e9 enregistr\u00e9es dans la m\u00e9moire de l'agent. Ce qui nous int\u00e9resse en fin de compte, c'est de savoir quelles actions sont les meilleures par rapport aux autres actions entreprises dans cet \u00e9pisode. Nous normaliserons donc nos r\u00e9compenses calcul\u00e9es en utilisant la moyenne et l'\u00e9cart type des r\u00e9compenses de l'\u00e9pisode d'apprentissage.\n", "\n", "Nous utiliserons cette d\u00e9finition de la fonction de r\u00e9compense dans les deux parties du laboratoire, alors assurez-vous de l'avoir ex\u00e9cut\u00e9e !"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "5_Q2OFYtQ32X"}, "outputs": [], "source": ["### Reward function ###\n", "\n", "# Helper function that normalizes an np.array x\n", "def normalize(x):\n", "    x -= np.mean(x)\n", "    x /= np.std(x)\n", "    return x.astype(np.float32)\n", "\n", "# Compute normalized, discounted, cumulative rewards (i.e., return)\n", "# Arguments:\n", "#   rewards: reward at timesteps in episode\n", "#   gamma: discounting factor\n", "# Returns:\n", "#   normalized discounted reward\n", "def discount_rewards(rewards, gamma=0.95): \n", "    discounted_rewards = np.zeros_like(rewards)\n", "    R = 0\n", "    for t in reversed(range(0, len(rewards))):\n", "        # update the total discounted reward\n", "        R = R * gamma + rewards[t]\n", "        discounted_rewards[t] = R\n", "      \n", "    return normalize(discounted_rewards)"]}, {"cell_type": "markdown", "metadata": {"id": "QzbY-mjGYcmt"}, "source": ["## 4.Algorithme d'apprentissage\n", "\n", "Maintenant nous pouvons commencer \u00e0 d\u00e9finir l'algorithme d'apprentissage qui sera utilis\u00e9 pour renforcer les bons comportements de l'agent et d\u00e9courager les mauvais comportements. Dans ce laboratoire, nous nous concentrerons sur les m\u00e9thodes de gradient de politique qui visent \u00e0 **maximiser** la probabilit\u00e9 d'actions qui r\u00e9sultent en de grandes r\u00e9compenses. De mani\u00e8re \u00e9quivalente, cela signifie que nous voulons **minimiser** la probabilit\u00e9 n\u00e9gative de ces m\u00eames actions. Nous y parvenons en **scalant** simplement les probabilit\u00e9s par les r\u00e9compenses qui leur sont associ\u00e9es, ce qui amplifie effectivement la probabilit\u00e9 des actions qui donnent lieu \u00e0 des r\u00e9compenses importantes.\n", "\n", "Puisque la fonction logarithmique est monotone et croissante, cela signifie que minimiser la **vraisemblance n\u00e9gative** est \u00e9quivalent \u00e0 minimiser la **vraisemblance logarithmique n\u00e9gative**.  Rappelons que nous pouvons facilement calculer la log-vraisemblance n\u00e9gative d'une action discr\u00e8te en \u00e9valuant son [entropie crois\u00e9e softmax] (https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). Comme dans l'apprentissage supervis\u00e9, nous pouvons utiliser des m\u00e9thodes de descente de gradient stochastique pour obtenir la minimisation souhait\u00e9e. \n", "\n", "Commen\u00e7ons par d\u00e9finir la fonction de perte."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "fsgZ3IDCY_Zn"}, "outputs": [], "source": ["### Loss function ###\n", "\n", "# Arguments:\n", "#   logits: network's predictions for actions to take\n", "#   actions: the actions the agent took in an episode\n", "#   rewards: the rewards the agent received in an episode\n", "# Returns:\n", "#   loss\n", "def compute_loss(logits, actions, rewards): \n", "    '''TODO: complete the function call to compute the negative log probabilities'''\n", "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n", "        logits=logits, labels=actions) # TODO\n", "    # neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n", "    #    logits='''TODO''', labels='''TODO''')\n", "  \n", "    '''TODO: scale the negative log probability by the rewards'''\n", "    loss = tf.reduce_mean( neg_logprob * rewards ) # TODO\n", "    # loss = tf.reduce_mean('''TODO''')\n", "    return loss"]}, {"cell_type": "markdown", "metadata": {"id": "Rr5vQ9fqbPpp"}, "source": ["Utilisons maintenant la fonction de perte pour d\u00e9finir une \u00e9tape de formation de notre algorithme d'apprentissage. Voici une d\u00e9finition tr\u00e8s g\u00e9n\u00e9ralisable que nous allons utiliser "]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "_50ada7nbZ7L"}, "outputs": [], "source": ["### Training step (forward and backpropagation) ###\n", "\n", "def train_step(model, loss_function, optimizer, observations, actions, discounted_rewards, custom_fwd_fn=None):\n", "    with tf.GradientTape() as tape:\n", "        # Forward propagate through the agent network\n", "        if custom_fwd_fn is not None:\n", "            prediction = custom_fwd_fn(observations)\n", "        else: \n", "            prediction = model(observations)\n", "\n", "        '''TODO: call the compute_loss function to compute the loss'''\n", "        loss = loss_function(prediction, actions, discounted_rewards) # TODO\n", "        # loss = loss_function('''TODO''', '''TODO''', '''TODO''')\n", "\n", "    '''TODO: run backpropagation to minimize the loss using the tape.gradient method. \n", "             Unlike supervised learning, RL is *extremely* noisy, so you will benefit \n", "             from additionally clipping your gradients to avoid falling into \n", "             dangerous local minima. After computing your gradients try also clipping\n", "             by a global normalizer. Try different clipping values, usually clipping \n", "             between 0.5 and 5 provides reasonable results. '''\n", "    grads = tape.gradient(loss, model.trainable_variables) # TODO\n", "    # grads = tape.gradient('''TODO''', '''TODO''')\n", "    grads, _ = tf.clip_by_global_norm(grads, 2)\n", "    # grads, _ = tf.clip_by_global_norm(grads, '''TODO''')\n", "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"]}, {"cell_type": "markdown", "metadata": {"id": "XsjKXh6BcgjR"}, "source": ["## 5 Ex\u00e9cuter cartpole !\n", "\n", "N'ayant aucune connaissance pr\u00e9alable de l'environnement, l'agent va commencer \u00e0 apprendre comment \u00e9quilibrer la perche sur le chariot en se basant uniquement sur les informations re\u00e7ues de l'environnement ! Apr\u00e8s avoir d\u00e9fini comment notre agent peut se d\u00e9placer, comment il re\u00e7oit de nouvelles observations et comment il met \u00e0 jour son \u00e9tat, nous allons voir comment il apprend progressivement une politique d'actions pour optimiser l'\u00e9quilibre du poteau aussi longtemps que possible. Pour ce faire, nous suivrons l'\u00e9volution des r\u00e9compenses en fonction de l'entra\u00eenement - comment les r\u00e9compenses devraient-elles changer au fur et \u00e0 mesure de l'entra\u00eenement ?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "-hZ7E6JOJ9Bn"}, "outputs": [], "source": ["## Training parameters ##\n", "## Re-run this cell to restart training from scratch ##\n", "\n", "# TODO: Learning rate and optimizer\n", "learning_rate = 1e-3\n", "# learning_rate = '''TODO'''\n", "optimizer = tf.keras.optimizers.Adam(learning_rate)\n", "# optimizer = '''TODO'''\n", "\n", "# instantiate cartpole agent\n", "cartpole_model = create_cartpole_model()\n", "\n", "# to track our progress\n", "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.95)\n", "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Rewards')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "XmOzc2rrcn8Q", "scrolled": false}, "outputs": [], "source": ["## Cartpole training! ##\n", "## Note: stoping and restarting this cell will pick up training where you\n", "#        left off. To restart training you need to rerun the cell above as \n", "#        well (to re-initialize the model and optimizer)\n", "\n", "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n", "for i_episode in range(500):\n", "\n", "    plotter.plot(smoothed_reward.get())\n", "    # Restart the environment\n", "    observation = env.reset()\n", "    memory.clear()\n", "\n", "    while True:\n", "        # using our observation, choose an action and take it in the environment\n", "        action = choose_action(cartpole_model, observation)\n", "        next_observation, reward, done, info = env.step(action)\n", "        # add to memory\n", "        memory.add_to_memory(observation, action, reward)\n", "\n", "        # is the episode over? did you crash or do so well that you're done?\n", "        if done:\n", "            # determine total reward and keep a record of this\n", "            total_reward = sum(memory.rewards)\n", "            smoothed_reward.append(total_reward)\n", "          \n", "            # initiate training - remember we don't know anything about how the \n", "            #   agent is doing until it has crashed!\n", "            g = train_step(cartpole_model, compute_loss, optimizer, \n", "                       observations=np.vstack(memory.observations),\n", "                       actions=np.array(memory.actions),\n", "                       discounted_rewards = discount_rewards(memory.rewards))\n", "          \n", "            # reset the memory\n", "            memory.clear()\n", "            break\n", "        # update our observatons\n", "        observation = next_observation"]}, {"cell_type": "markdown", "metadata": {"id": "mkcUtGF1VE-K"}, "source": ["Pour avoir une id\u00e9e de la fa\u00e7on dont notre agent s'est d\u00e9brouill\u00e9, nous pouvons enregistrer une vid\u00e9o du mod\u00e8le entra\u00een\u00e9 travaillant sur l'\u00e9quilibre du poteau. Il s'agit d'un tout nouvel environnement que l'agent n'a jamais vu auparavant !\n", "\n", "Affichons la vid\u00e9o enregistr\u00e9e pour voir comment notre agent s'est comport\u00e9 !"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "PAYBkv6Zbk0J", "scrolled": true}, "outputs": [], "source": ["matplotlib.use('Agg') \n", "saved_cartpole = mdl.lab3.save_video_of_model(cartpole_model, \"CartPole-v1\")\n", "mdl.lab3.play_video(saved_cartpole)"]}, {"cell_type": "markdown", "metadata": {"id": "CSbVNDpaVb3_"}, "source": ["Quelles sont les performances de l'agent ? Pourriez-vous le former pendant une p\u00e9riode plus courte tout en obtenant de bons r\u00e9sultats ? Pensez-vous qu'un entrainement plus long serait encore plus efficace ? "]}, {"cell_type": "code", "source": [""], "metadata": {"id": "x_wlW0USt8y8"}, "execution_count": null, "outputs": []}], "metadata": {"accelerator": "GPU", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 0}