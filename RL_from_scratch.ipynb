{"cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github", "colab_type": "text"}, "source": ["<a href=\"https://colab.research.google.com/github/yahia-kplr/Machine-Learning/blob/main/RL_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "markdown", "id": "3d4763a2", "metadata": {"id": "3d4763a2"}, "source": ["# Reinforcement Learning\n", "### Objectif\n", "- Construire votre propre mod\u00e8le d'apprentissage automatique \u00e0 partir de z\u00e9ro - apprentissage par renforcement.\n", "- Apprenez-lui \u00e0 ramasser et \u00e0 livrer des articles."]}, {"cell_type": "markdown", "id": "2e087817", "metadata": {"id": "2e087817"}, "source": ["### Qu'est-ce que l'apprentissage par renforcement ?\n", "- L'apprentissage par renforcement apprend \u00e0 la machine \u00e0 penser par elle-m\u00eame en fonction des r\u00e9compenses des actions pass\u00e9es.\n", "![Apprentissage par renforcement](https://github.com/LearnPythonWithRune/LearnPython/blob/main/colab/final/img/ReinforcementLearning.png?raw=1)\n", "- En gros, l'algorithme d'apprentissage par renforcement essaie de pr\u00e9dire les actions qui donnent des r\u00e9compenses et \u00e9vitent les punitions.\n", "- C'est comme le dressage d'un chien. Vous et le chien ne parlez pas la m\u00eame langue, mais le chien apprend \u00e0 agir en fonction des r\u00e9compenses (et des punitions, que je ne conseille pas et ne pr\u00e9conise pas).\n", "- Ainsi, si un chien est r\u00e9compens\u00e9 pour une certaine action dans une situation donn\u00e9e, la prochaine fois qu'il sera expos\u00e9 \u00e0 une situation similaire, il agira de la m\u00eame mani\u00e8re.\n", "- Traduisez cela en apprentissage par renforcement.\n", "    - L' **agent** est le chien qui est expos\u00e9 \u00e0 l' **environnement**.\n", "    - Puis **l'agent** rencontre un **\u00e9tat**.\n", "    - **L'agent** ex\u00e9cute une **action** pour passer \u00e0 un **nouvel \u00e9tat**.\n", "    - Puis, apr\u00e8s la transition, l'agent** re\u00e7oit une **r\u00e9compense** ou une **p\u00e9nalit\u00e9 (punition)**.\n", "    - Cela forme une **politique** pour cr\u00e9er une strat\u00e9gie permettant de choisir des actions dans un **\u00e9tat** donn\u00e9."]}, {"cell_type": "markdown", "id": "7d23849c", "metadata": {"id": "7d23849c"}, "source": ["#### Quels sont les algorithmes utilis\u00e9s pour l'apprentissage par renforcement ?\n", "- Les algorithmes les plus courants pour l'apprentissage par renforcement sont.\n", "    - **Q-Learning** : est un algorithme d'apprentissage par renforcement sans mod\u00e8le pour apprendre une politique indiquant \u00e0 un agent quelle action entreprendre dans quelles circonstances.\n", "    - **Diff\u00e9rence temporelle** : fait r\u00e9f\u00e9rence \u00e0 une classe de m\u00e9thodes d'apprentissage par renforcement sans mod\u00e8le qui apprennent par bootstrapping \u00e0 partir de l'estimation actuelle de la fonction de valeur.\n", "    - **Deep Adversarial Network** : est une technique employ\u00e9e dans le domaine de l'apprentissage automatique qui tente de tromper les mod\u00e8les par des entr\u00e9es malveillantes.\n", "- Nous nous concentrerons sur l'algorithme **Q-learning** car il est \u00e0 la fois facile \u00e0 comprendre et puissant."]}, {"cell_type": "markdown", "id": "996b6b0b", "metadata": {"id": "996b6b0b"}, "source": ["### Comment fonctionne l'algorithme d'apprentissage Q ?\n", "- Comme je l'ai d\u00e9j\u00e0 dit, j'adore cet algorithme. Il est \"facile\" \u00e0 comprendre et puissant comme vous allez le voir.\n", "![Algorithme Q-Learning](https://github.com/LearnPythonWithRune/LearnPython/blob/main/colab/final/img/QLearning.png?raw=1)\n", "- L'algorithme **Q-Learning** a une **Q-table** (une matrice de dimension \u00e9tat x actions - ne vous inqui\u00e9tez pas si vous ne comprenez pas ce qu'est une matrice, vous n'aurez pas besoin des aspects math\u00e9matiques de celle-ci - c'est juste un \"conteneur\" index\u00e9 avec des nombres).\n", "- Le **agent** (ou l'algorithme **Q-Learning**) sera dans un \u00e9tat.\n", "- Ensuite, \u00e0 chaque it\u00e9ration, le **agent** doit prendre une **action**.\n", "- **L'agent** met continuellement \u00e0 jour la **r\u00e9compense** dans la **table Q**.\n", "- L'apprentissage peut provenir soit de l'exploitation, soit de l'exploration.\n", "- Cela se traduit par le pseudo-algorithme suivant pour le **Q-Learning**.\n", "- L' **agent** est dans un **\u00e9tat\u00ba\u00ba donn\u00e9 et doit choisir une **action**.\n", "\n", "#### Algorithme\n", "- Initialiser la **table Q** \u00e0 tous les z\u00e9ros.\n", "- It\u00e9ration\n", "    - L'agent est dans l'\u00e9tat **\u00e9tat**.\n", "    - Avec la probabilit\u00e9 **epsilon**, il choisit **explorer**, sinon **exploiter**.\n", "        - Si **explorer**, alors choisir une **action** *al\u00e9atoire*.\n", "        - Si **exploiter**, alors choisissez la **meilleure* action** sur la base de la **table Q** actuelle.\n", "    - Mettez \u00e0 jour la **table Q** de la nouvelle **r\u00e9compense** \u00e0 l'\u00e9tat pr\u00e9c\u00e9dent.\n", "    - Q[**\u00e9tat, action**] = (1 - **alpha**) * Q[**\u00e9tat, action**] + **alpha** * (**r\u00e9compense + gamma** * max(Q[**nouvel_\u00e9tat**]) - Q[**\u00e9tat, action**])\n", "    \n", "#### Variables\n", "Comme vous pouvez le constater, nous avons introduit les variables suivantes.\n", "\n", "- **epsilon** : la probabilit\u00e9 de prendre une action al\u00e9atoire, qui est faite pour explorer un nouveau territoire.\n", "- **alpha** : est le taux d'apprentissage que l'algorithme doit faire \u00e0 chaque it\u00e9ration et doit \u00eatre dans l'intervalle de 0 \u00e0 1.\n", "- **gamma** : est le facteur d'actualisation utilis\u00e9 pour \u00e9quilibrer la r\u00e9compense imm\u00e9diate et future. Cette valeur est g\u00e9n\u00e9ralement comprise entre 0,8 et 0,99.\n", "- **R\u00e9compense** : est la r\u00e9troaction sur l'action et peut \u00eatre un nombre quelconque. Une valeur n\u00e9gative correspond \u00e0 une p\u00e9nalit\u00e9 (ou une punition) et une valeur positive \u00e0 une r\u00e9compense."]}, {"cell_type": "markdown", "id": "f8b7a709", "metadata": {"id": "f8b7a709"}, "source": ["### Description de la t\u00e2che\n", "- To keep it simple, we create a field of size 10\u00d710 positions. In that field there is an item that needs to be picked up and moved to a drop-off point.\n", "![Field](https://github.com/LearnPythonWithRune/LearnPython/blob/main/colab/final/img/Field-ReinforcementLearning.png?raw=1)\n", "- At each position there are 6 different actions that can be taken.\n", "    - **Action 0**: Go South if on field.\n", "    - **Action 1**: Go North if on field.\n", "    - **Action 2**: Go East if on field (Please notice, I mixed up East and West (East is Left here)).\n", "    - **Action 3**: Go West if on field (Please notice, I mixed up East and West (West is right here)).\n", "\n", "    - **Action 4**: Pickup item (it can try even if it is not there)\n", "    - **Action 5**: Drop-off item (it can try even if it does not have it)\n", "- Based on these actions we will make a reward system.\n", "    - If the **agent** tries to go off the **field**, punish with **-10** in **reward**.\n", "    - If the **agent** makes a (legal) move, punish with **-1** in **reward**, as we do not want to encourage endless walking around.\n", "    - If the **agent** tries to pick up item, but it is not there or it has it already, punish with **-10** in **reward**.\n", "    - If the **agent** picks up the item correct place, **reward** with **20**.\n", "    - If **agent** tries to drop-off item in wrong place or does not have the item, punish with **-10** in **reward**.\n", "    - If the **agent** drops-off item in correct place, **reward** with **20**.\n", "- That translates into the following code. I prefer to implement this code, as I think the standard libraries that provide similar frameworks hide some important details. As an example, and shown later, how do you map this into a state in the Q-table?"]}, {"cell_type": "code", "execution_count": null, "id": "9626458a", "metadata": {"id": "9626458a"}, "outputs": [], "source": ["class Field:\n", "    def __init__(self, size, item_pickup, item_drop_off, start_position):\n", "        self.size = size\n", "        self.item_pickup = item_pickup\n", "        self.item_drop_off = item_drop_off\n", "        self.position = start_position\n", "        self.item_in_car = False\n", "        \n", "    def get_number_of_states(self):\n", "        return self.size*self.size*self.size*self.size*2\n", "    \n", "    def get_state(self):\n", "        state = self.position[0]*self.size*self.size*self.size*2\n", "        state = state + self.position[1]*self.size*self.size*2\n", "        state = state + self.item_pickup[0]*self.size*2\n", "        state = state + self.item_pickup[1]*2\n", "        if self.item_in_car:\n", "            state = state + 1\n", "        return state\n", "        \n", "    def make_action(self, action):\n", "        (x, y) = self.position\n", "        if action == 0:  # Go South\n", "            if y == self.size - 1:\n", "                return -10, False\n", "            else:\n", "                self.position = (x, y + 1)\n", "                return -1, False\n", "        elif action == 1:  # Go North\n", "            if y == 0:\n", "                return -10, False\n", "            else:\n", "                self.position = (x, y - 1)\n", "                return -1, False\n", "        elif action == 2:  # Go East\n", "            if x == 0:\n", "                return -10, False\n", "            else:\n", "                self.position = (x - 1, y)\n", "                return -1, False\n", "        elif action == 3:  # Go West\n", "            if x == self.size - 1:\n", "                return -10, False\n", "            else:\n", "                self.position = (x + 1, y)\n", "                return -1, False\n", "        elif action == 4:  # Pickup item\n", "            if self.item_in_car:\n", "                return -10, False\n", "            elif self.item_pickup != (x, y):\n", "                return -10, False\n", "            else:\n", "                self.item_in_car = True\n", "                return 20, False\n", "        elif action == 5:  # Drop off item\n", "            if not self.item_in_car:\n", "                return -10, False\n", "            elif self.item_drop_off != (x, y):\n", "                self.item_pickup = (x, y)\n", "                self.item_in_car = False\n", "                return -10, False\n", "            else:\n", "                return 20, True\n", "        \n", "        "]}, {"cell_type": "code", "execution_count": null, "id": "26a464db", "metadata": {"id": "26a464db", "outputId": "c37e67c3-2486-4793-dd23-21e719f29296", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["(9, 9)"]}, "metadata": {}, "execution_count": 15}], "source": ["field = Field(10, (0, 0), (9, 9), (9, 0))\n", "\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "field.make_action(2)\n", "\n", "field.make_action(4)\n", "\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "field.make_action(0)\n", "\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "field.make_action(3)\n", "\n", "field.make_action(5)\n", "\n", "field.position"]}, {"cell_type": "code", "execution_count": null, "id": "c777c0f5", "metadata": {"id": "c777c0f5"}, "outputs": [], "source": ["import random"]}, {"cell_type": "code", "execution_count": null, "id": "bd4babdc", "metadata": {"id": "bd4babdc"}, "outputs": [], "source": ["def naive_solution():\n", "    size = 10\n", "    item_start = (0, 0)\n", "    item_drop_off = (9, 9)\n", "    start_position = (0, 9)\n", "    \n", "    field = Field(size, item_start, item_drop_off, start_position)\n", "    done = False\n", "    steps = 0\n", "    \n", "    while not done:\n", "        action = random.randint(0, 5)\n", "        reward, done = field.make_action(action)\n", "        steps = steps + 1\n", "    \n", "    return steps"]}, {"cell_type": "code", "execution_count": null, "id": "2a90e844", "metadata": {"id": "2a90e844", "outputId": "0eebd347-e5e4-48c5-d085-04162c93fc9e", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["51937"]}, "metadata": {}, "execution_count": 18}], "source": ["naive_solution()"]}, {"cell_type": "code", "execution_count": null, "id": "832e6038", "metadata": {"id": "832e6038"}, "outputs": [], "source": ["runs = [naive_solution() for _ in range(100)]"]}, {"cell_type": "code", "execution_count": null, "id": "06e4ac99", "metadata": {"scrolled": false, "id": "06e4ac99", "outputId": "fadf99b5-f372-41d8-e5b4-731eddc9031d", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["122393.87"]}, "metadata": {}, "execution_count": 7}], "source": ["sum(runs)/len(runs)"]}, {"cell_type": "markdown", "id": "6d15cc65", "metadata": {"id": "6d15cc65"}, "source": ["#### Algorithme\n", "- Initialiser la **Table Q** \u00e0 tous les z\u00e9ros\n", "- It\u00e9ration\n", "    - L'agent est dans l'\u00e9tat **\u00e9tat**.\n", "    - Avec la probabilit\u00e9 **epsilon**, il choisit **explorer**, sinon **exploiter**.\n", "        - Si **explorer**, alors choisir une **action** *al\u00e9atoire*.\n", "        - Si **exploiter**, alors choisissez la **meilleure* action** sur la base de la **table Q** actuelle.\n", "    - Mettez \u00e0 jour la **table Q** de la nouvelle **r\u00e9compense** \u00e0 l'\u00e9tat pr\u00e9c\u00e9dent.\n", "    - Q[**\u00e9tat, action**] = (1 - **alpha**) * Q[**\u00e9tat, action**] + **alpha** * (**r\u00e9compense + gamma** * max(Q[**nouvel_\u00e9tat**]) - Q[**\u00e9tat, action**])"]}, {"cell_type": "code", "execution_count": null, "id": "51f89ae8", "metadata": {"id": "51f89ae8"}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": null, "id": "fb9fe412", "metadata": {"id": "fb9fe412"}, "outputs": [], "source": ["size = 10\n", "item_start = (0, 0)\n", "item_drop_off = (9, 9)\n", "start_position = (0, 9)\n", "\n", "field = Field(size, item_start, item_drop_off, start_position)\n", "\n", "number_of_states = field.get_number_of_states()\n", "number_of_actions = 6\n", "\n", "q_table = np.zeros((number_of_states, number_of_actions))\n", "\n", "epsilon = 0.1\n", "alpha = 0.1\n", "gamma = 0.6\n", "\n", "for _ in range(10000):\n", "    field = Field(size, item_start, item_drop_off, start_position)\n", "    done = False\n", "    \n", "    while not done:\n", "        state = field.get_state()\n", "        if random.uniform(0, 1) < epsilon:\n", "            action = random.randint(0, 5)\n", "        else:\n", "            action = np.argmax(q_table[state])\n", "            \n", "        reward, done = field.make_action(action)\n", "        # Q[state, action] = (1 \u2013 alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) \u2014 Q[state, action])\n", "        \n", "        new_state = field.get_state()\n", "        new_state_max = np.max(q_table[new_state])\n", "        \n", "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])"]}, {"cell_type": "code", "execution_count": null, "id": "a49cfff8", "metadata": {"id": "a49cfff8"}, "outputs": [], "source": ["def reinforcement_learning():\n", "    epsilon = 0.1\n", "    alpha = 0.1\n", "    gamma = 0.6\n", "    \n", "    field = Field(size, item_start, item_drop_off, start_position)\n", "    done = False\n", "    steps = 0\n", "    \n", "    while not done:\n", "        state = field.get_state()\n", "        if random.uniform(0, 1) < epsilon:\n", "            action = random.randint(0, 5)\n", "        else:\n", "            action = np.argmax(q_table[state])\n", "            \n", "        reward, done = field.make_action(action)\n", "        # Q[state, action] = (1 \u2013 alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) \u2014 Q[state, action])\n", "        \n", "        new_state = field.get_state()\n", "        new_state_max = np.max(q_table[new_state])\n", "        \n", "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])\n", "        \n", "        steps = steps + 1\n", "    \n", "    return steps"]}, {"cell_type": "code", "execution_count": null, "id": "c6932ae1", "metadata": {"id": "c6932ae1", "outputId": "25769418-cba0-4a42-ef51-fdf0e646fd91", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["166"]}, "metadata": {}, "execution_count": 11}], "source": ["reinforcement_learning()"]}, {"cell_type": "code", "execution_count": null, "id": "60d9f3dc", "metadata": {"id": "60d9f3dc"}, "outputs": [], "source": ["runs_rl = [reinforcement_learning() for _ in range(100)]"]}, {"cell_type": "code", "execution_count": null, "id": "9927eec1", "metadata": {"id": "9927eec1", "outputId": "4303bdb9-e70b-477f-f58f-2fbc033999d5", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["44.76"]}, "metadata": {}, "execution_count": 13}], "source": ["sum(runs_rl)/len(runs_rl)"]}], "metadata": {"language_info": {"name": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "gpuClass": "standard"}, "nbformat": 4, "nbformat_minor": 5}