{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi0101/ML-TPs/blob/main/RL_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4763a2",
      "metadata": {
        "id": "3d4763a2"
      },
      "source": [
        "# Reinforcement Learning\n",
        "### Objectif\n",
        "- Construire votre propre modèle d'apprentissage automatique à partir de zéro - apprentissage par renforcement.\n",
        "- Apprenez-lui à ramasser et à livrer des articles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e087817",
      "metadata": {
        "id": "2e087817"
      },
      "source": [
        "### Qu'est-ce que l'apprentissage par renforcement ?\n",
        "- L'apprentissage par renforcement apprend à la machine à penser par elle-même en fonction des récompenses des actions passées.\n",
        "![Apprentissage par renforcement](https://github.com/LearnPythonWithRune/LearnPython/blob/main/colab/final/img/ReinforcementLearning.png?raw=1)\n",
        "- En gros, l'algorithme d'apprentissage par renforcement essaie de prédire les actions qui donnent des récompenses et évitent les punitions.\n",
        "- C'est comme le dressage d'un chien. Vous et le chien ne parlez pas la même langue, mais le chien apprend à agir en fonction des récompenses (et des punitions, que je ne conseille pas et ne préconise pas).\n",
        "- Ainsi, si un chien est récompensé pour une certaine action dans une situation donnée, la prochaine fois qu'il sera exposé à une situation similaire, il agira de la même manière.\n",
        "- Traduisez cela en apprentissage par renforcement.\n",
        "    - L' **agent** est le chien qui est exposé à l' **environnement**.\n",
        "    - Puis **l'agent** rencontre un **état**.\n",
        "    - **L'agent** exécute une **action** pour passer à un **nouvel état**.\n",
        "    - Puis, après la transition, l'agent** reçoit une **récompense** ou une **pénalité (punition)**.\n",
        "    - Cela forme une **politique** pour créer une stratégie permettant de choisir des actions dans un **état** donné."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d23849c",
      "metadata": {
        "id": "7d23849c"
      },
      "source": [
        "#### Quels sont les algorithmes utilisés pour l'apprentissage par renforcement ?\n",
        "- Les algorithmes les plus courants pour l'apprentissage par renforcement sont.\n",
        "    - **Q-Learning** : est un algorithme d'apprentissage par renforcement sans modèle pour apprendre une politique indiquant à un agent quelle action entreprendre dans quelles circonstances.\n",
        "    - **Différence temporelle** : fait référence à une classe de méthodes d'apprentissage par renforcement sans modèle qui apprennent par bootstrapping à partir de l'estimation actuelle de la fonction de valeur.\n",
        "    - **Deep Adversarial Network** : est une technique employée dans le domaine de l'apprentissage automatique qui tente de tromper les modèles par des entrées malveillantes.\n",
        "- Nous nous concentrerons sur l'algorithme **Q-learning** car il est à la fois facile à comprendre et puissant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996b6b0b",
      "metadata": {
        "id": "996b6b0b"
      },
      "source": [
        "### Comment fonctionne l'algorithme d'apprentissage Q ?\n",
        "- Comme je l'ai déjà dit, j'adore cet algorithme. Il est \"facile\" à comprendre et puissant comme vous allez le voir.\n",
        "![Algorithme Q-Learning](https://github.com/LearnPythonWithRune/LearnPython/blob/main/colab/final/img/QLearning.png?raw=1)\n",
        "- L'algorithme **Q-Learning** a une **Q-table** (une matrice de dimension état x actions - ne vous inquiétez pas si vous ne comprenez pas ce qu'est une matrice, vous n'aurez pas besoin des aspects mathématiques de celle-ci - c'est juste un \"conteneur\" indexé avec des nombres).\n",
        "- Le **agent** (ou l'algorithme **Q-Learning**) sera dans un état.\n",
        "- Ensuite, à chaque itération, le **agent** doit prendre une **action**.\n",
        "- **L'agent** met continuellement à jour la **récompense** dans la **table Q**.\n",
        "- L'apprentissage peut provenir soit de l'exploitation, soit de l'exploration.\n",
        "- Cela se traduit par le pseudo-algorithme suivant pour le **Q-Learning**.\n",
        "- L' **agent** est dans un **étatºº donné et doit choisir une **action**.\n",
        "\n",
        "#### Algorithme\n",
        "- Initialiser la **table Q** à tous les zéros.\n",
        "- Itération\n",
        "    - L'agent est dans l'état **état**.\n",
        "    - Avec la probabilité **epsilon**, il choisit **explorer**, sinon **exploiter**.\n",
        "        - Si **explorer**, alors choisir une **action** *aléatoire*.\n",
        "        - Si **exploiter**, alors choisissez la **meilleure* action** sur la base de la **table Q** actuelle.\n",
        "    - Mettez à jour la **table Q** de la nouvelle **récompense** à l'état précédent.\n",
        "    - Q[**état, action**] = (1 - **alpha**) * Q[**état, action**] + **alpha** * (**récompense + gamma** * max(Q[**nouvel_état**]) - Q[**état, action**])\n",
        "    \n",
        "#### Variables\n",
        "Comme vous pouvez le constater, nous avons introduit les variables suivantes.\n",
        "\n",
        "- **epsilon** : la probabilité de prendre une action aléatoire, qui est faite pour explorer un nouveau territoire.\n",
        "- **alpha** : est le taux d'apprentissage que l'algorithme doit faire à chaque itération et doit être dans l'intervalle de 0 à 1.\n",
        "- **gamma** : est le facteur d'actualisation utilisé pour équilibrer la récompense immédiate et future. Cette valeur est généralement comprise entre 0,8 et 0,99.\n",
        "- **Récompense** : est la rétroaction sur l'action et peut être un nombre quelconque. Une valeur négative correspond à une pénalité (ou une punition) et une valeur positive à une récompense."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b7a709",
      "metadata": {
        "id": "f8b7a709"
      },
      "source": [
        "### Description de la tâche\n",
        "- To keep it simple, we create a field of size 10×10 positions. In that field there is an item that needs to be picked up and moved to a drop-off point.\n",
        "![Field](https://github.com/LearnPythonWithRune/LearnPython/blob/main/colab/final/img/Field-ReinforcementLearning.png?raw=1)\n",
        "- At each position there are 6 different actions that can be taken.\n",
        "    - **Action 0**: Go South if on field.\n",
        "    - **Action 1**: Go North if on field.\n",
        "    - **Action 2**: Go East if on field (Please notice, I mixed up East and West (East is Left here)).\n",
        "    - **Action 3**: Go West if on field (Please notice, I mixed up East and West (West is right here)).\n",
        "\n",
        "    - **Action 4**: Pickup item (it can try even if it is not there)\n",
        "    - **Action 5**: Drop-off item (it can try even if it does not have it)\n",
        "- Based on these actions we will make a reward system.\n",
        "    - If the **agent** tries to go off the **field**, punish with **-10** in **reward**.\n",
        "    - If the **agent** makes a (legal) move, punish with **-1** in **reward**, as we do not want to encourage endless walking around.\n",
        "    - If the **agent** tries to pick up item, but it is not there or it has it already, punish with **-10** in **reward**.\n",
        "    - If the **agent** picks up the item correct place, **reward** with **20**.\n",
        "    - If **agent** tries to drop-off item in wrong place or does not have the item, punish with **-10** in **reward**.\n",
        "    - If the **agent** drops-off item in correct place, **reward** with **20**.\n",
        "- That translates into the following code. I prefer to implement this code, as I think the standard libraries that provide similar frameworks hide some important details. As an example, and shown later, how do you map this into a state in the Q-table?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9626458a",
      "metadata": {
        "id": "9626458a"
      },
      "outputs": [],
      "source": [
        "class Field:\n",
        "    def __init__(self, size, item_pickup, item_drop_off, start_position):\n",
        "        self.size = size\n",
        "        self.item_pickup = item_pickup\n",
        "        self.item_drop_off = item_drop_off\n",
        "        self.position = start_position\n",
        "        self.item_in_car = False\n",
        "        \n",
        "    def get_number_of_states(self):\n",
        "        return self.size*self.size*self.size*self.size*2\n",
        "    \n",
        "    def get_state(self):\n",
        "        state = self.position[0]*self.size*self.size*self.size*2\n",
        "        state = state + self.position[1]*self.size*self.size*2\n",
        "        state = state + self.item_pickup[0]*self.size*2\n",
        "        state = state + self.item_pickup[1]*2\n",
        "        if self.item_in_car:\n",
        "            state = state + 1\n",
        "        return state\n",
        "        \n",
        "    def make_action(self, action):\n",
        "        (x, y) = self.position\n",
        "        if action == 0:  # Go South\n",
        "            if y == self.size - 1:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x, y + 1)\n",
        "                return -1, False\n",
        "        elif action == 1:  # Go North\n",
        "            if y == 0:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x, y - 1)\n",
        "                return -1, False\n",
        "        elif action == 2:  # Go East\n",
        "            if x == 0:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x - 1, y)\n",
        "                return -1, False\n",
        "        elif action == 3:  # Go West\n",
        "            if x == self.size - 1:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x + 1, y)\n",
        "                return -1, False\n",
        "        elif action == 4:  # Pickup item\n",
        "            if self.item_in_car:\n",
        "                return -10, False\n",
        "            elif self.item_pickup != (x, y):\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.item_in_car = True\n",
        "                return 20, False\n",
        "        elif action == 5:  # Drop off item\n",
        "            if not self.item_in_car:\n",
        "                return -10, False\n",
        "            elif self.item_drop_off != (x, y):\n",
        "                self.item_pickup = (x, y)\n",
        "                self.item_in_car = False\n",
        "                return -10, False\n",
        "            else:\n",
        "                return 20, True\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a464db",
      "metadata": {
        "id": "26a464db",
        "outputId": "c37e67c3-2486-4793-dd23-21e719f29296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "field = Field(10, (0, 0), (9, 9), (9, 0))\n",
        "\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "\n",
        "field.make_action(4)\n",
        "\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "\n",
        "field.make_action(5)\n",
        "\n",
        "field.position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c777c0f5",
      "metadata": {
        "id": "c777c0f5"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4babdc",
      "metadata": {
        "id": "bd4babdc"
      },
      "outputs": [],
      "source": [
        "def naive_solution():\n",
        "    size = 10\n",
        "    item_start = (0, 0)\n",
        "    item_drop_off = (9, 9)\n",
        "    start_position = (0, 9)\n",
        "    \n",
        "    field = Field(size, item_start, item_drop_off, start_position)\n",
        "    done = False\n",
        "    steps = 0\n",
        "    \n",
        "    while not done:\n",
        "        action = random.randint(0, 5)\n",
        "        reward, done = field.make_action(action)\n",
        "        steps = steps + 1\n",
        "    \n",
        "    return steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a90e844",
      "metadata": {
        "id": "2a90e844",
        "outputId": "0eebd347-e5e4-48c5-d085-04162c93fc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51937"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "naive_solution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "832e6038",
      "metadata": {
        "id": "832e6038"
      },
      "outputs": [],
      "source": [
        "runs = [naive_solution() for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e4ac99",
      "metadata": {
        "scrolled": false,
        "id": "06e4ac99",
        "outputId": "fadf99b5-f372-41d8-e5b4-731eddc9031d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122393.87"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sum(runs)/len(runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d15cc65",
      "metadata": {
        "id": "6d15cc65"
      },
      "source": [
        "#### Algorithme\n",
        "- Initialiser la **Table Q** à tous les zéros\n",
        "- Itération\n",
        "    - L'agent est dans l'état **état**.\n",
        "    - Avec la probabilité **epsilon**, il choisit **explorer**, sinon **exploiter**.\n",
        "        - Si **explorer**, alors choisir une **action** *aléatoire*.\n",
        "        - Si **exploiter**, alors choisissez la **meilleure* action** sur la base de la **table Q** actuelle.\n",
        "    - Mettez à jour la **table Q** de la nouvelle **récompense** à l'état précédent.\n",
        "    - Q[**état, action**] = (1 - **alpha**) * Q[**état, action**] + **alpha** * (**récompense + gamma** * max(Q[**nouvel_état**]) - Q[**état, action**])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f89ae8",
      "metadata": {
        "id": "51f89ae8"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9fe412",
      "metadata": {
        "id": "fb9fe412"
      },
      "outputs": [],
      "source": [
        "size = 10\n",
        "item_start = (0, 0)\n",
        "item_drop_off = (9, 9)\n",
        "start_position = (0, 9)\n",
        "\n",
        "field = Field(size, item_start, item_drop_off, start_position)\n",
        "\n",
        "number_of_states = field.get_number_of_states()\n",
        "number_of_actions = 6\n",
        "\n",
        "q_table = np.zeros((number_of_states, number_of_actions))\n",
        "\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "\n",
        "for _ in range(10000):\n",
        "    field = Field(size, item_start, item_drop_off, start_position)\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        state = field.get_state()\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, 5)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "            \n",
        "        reward, done = field.make_action(action)\n",
        "        # Q[state, action] = (1 – alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) — Q[state, action])\n",
        "        \n",
        "        new_state = field.get_state()\n",
        "        new_state_max = np.max(q_table[new_state])\n",
        "        \n",
        "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49cfff8",
      "metadata": {
        "id": "a49cfff8"
      },
      "outputs": [],
      "source": [
        "def reinforcement_learning():\n",
        "    epsilon = 0.1\n",
        "    alpha = 0.1\n",
        "    gamma = 0.6\n",
        "    \n",
        "    field = Field(size, item_start, item_drop_off, start_position)\n",
        "    done = False\n",
        "    steps = 0\n",
        "    \n",
        "    while not done:\n",
        "        state = field.get_state()\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, 5)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "            \n",
        "        reward, done = field.make_action(action)\n",
        "        # Q[state, action] = (1 – alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) — Q[state, action])\n",
        "        \n",
        "        new_state = field.get_state()\n",
        "        new_state_max = np.max(q_table[new_state])\n",
        "        \n",
        "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])\n",
        "        \n",
        "        steps = steps + 1\n",
        "    \n",
        "    return steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6932ae1",
      "metadata": {
        "id": "c6932ae1",
        "outputId": "25769418-cba0-4a42-ef51-fdf0e646fd91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "reinforcement_learning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d9f3dc",
      "metadata": {
        "id": "60d9f3dc"
      },
      "outputs": [],
      "source": [
        "runs_rl = [reinforcement_learning() for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9927eec1",
      "metadata": {
        "id": "9927eec1",
        "outputId": "4303bdb9-e70b-477f-f58f-2fbc033999d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44.76"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "sum(runs_rl)/len(runs_rl)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "16 - Project - Capstone Project - Reinforcement Learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}